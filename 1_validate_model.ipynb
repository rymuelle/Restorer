{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6351e77",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "import copy\n",
    "import mlflow\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2043dc8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.training.SmallRawDataset import SmallRawDataset\n",
    "\n",
    "from src.training.losses.ShadowAwareLoss import ShadowAwareLoss\n",
    "from src.training.VGGFeatureExtractor import VGGFeatureExtractor\n",
    "from src.training.utils import apply_gamma_torch\n",
    "from src.training.train_loop import visualize\n",
    "from src.training.load_config import load_config\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b14af214",
   "metadata": {},
   "outputs": [],
   "source": [
    "run_config = load_config()\n",
    "dataset_path = Path(run_config['jpeg_output_subdir'])\n",
    "align_csv = dataset_path / run_config['secondary_align_csv']\n",
    "device=run_config['device']\n",
    "\n",
    "batch_size = run_config['batch_size']\n",
    "lr = run_config['lr_base'] * batch_size\n",
    "clipping =  run_config['clipping']\n",
    "\n",
    "num_epochs = run_config['num_epochs_pretraining']\n",
    "val_split = run_config['val_split']\n",
    "crop_size = run_config['crop_size']\n",
    "experiment = run_config['mlflow_experiment']\n",
    "model_params = run_config['model_params']\n",
    "rggb = model_params['rggb']\n",
    "mlflow_path = run_config['mlflow_path']\n",
    "mlflow.set_tracking_uri(f\"file://{mlflow_path}\")\n",
    "mlflow.set_experiment(experiment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba20b866",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "RUN_ID = \"b0664f324e9444d3b3a5277d513d3642\"  \n",
    "ARTIFACT_PATH = run_config['run_path']\n",
    "\n",
    "model_uri = f\"runs:/{RUN_ID}/{ARTIFACT_PATH}\"\n",
    "\n",
    "try:\n",
    "    model = mlflow.pytorch.load_model(model_uri)\n",
    "    model.eval()\n",
    "    print(f\"Model successfully loaded from MLflow URI: {model_uri}\")\n",
    "    \n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Error loading model from MLflow: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15f16fa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = SmallRawDataset(dataset_path, align_csv, crop_size=crop_size)\n",
    "\n",
    "# Split dataset into train and val\n",
    "val_size = int(len(dataset) * val_split)\n",
    "train_size = len(dataset) - val_size\n",
    "torch.manual_seed(42)  # For reproducibility\n",
    "train_dataset, val_dataset = random_split(dataset, [train_size, val_size])\n",
    "# Set the validation dataset to use the same crops\n",
    "val_dataset = copy.deepcopy(val_dataset)\n",
    "val_dataset.dataset.validation = True\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=False, num_workers=0)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6af0f3a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "vfe = VGGFeatureExtractor(config=((1, 64), (1, 128), (1, 256), (1, 512), (1, 512),), \n",
    "                          feature_layers=[14], \n",
    "                          activation=nn.ReLU\n",
    "                          )\n",
    "vfe = vfe.to(device)\n",
    "\n",
    "loss_fn = ShadowAwareLoss(\n",
    "    alpha=run_config['alpha'],\n",
    "    beta=run_config['beta'],\n",
    "    l1_weight=run_config['l1_weight'],\n",
    "    ssim_weight=run_config['ssim_weight'],\n",
    "    tv_weight=run_config['tv_weight'],\n",
    "    vgg_loss_weight=run_config['vgg_loss_weight'],\n",
    "    apply_gamma_fn=apply_gamma_torch,\n",
    "    vgg_feature_extractor=vfe,\n",
    "    device=device,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51ad2666",
   "metadata": {},
   "outputs": [],
   "source": [
    "# with mlflow.start_run(run_id=existing_run_id) as run:\n",
    "#     print(f\"Re-opened run: {run.info.run_name}\")\n",
    "    \n",
    "#     # Log your new validation metric\n",
    "#     mlflow.log_metric(\"final_validation_accuracy\", new_validation_metric)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae01182c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "subset_indices = np.array(val_dataset.indices)  # indices in the original dataset\n",
    "mask = val_dataset.dataset.df.iso.values[subset_indices] == 65535\n",
    "matching_indices_in_subset = np.nonzero(mask)[0]\n",
    "matching_indices_in_subset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2234a52d",
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize(matching_indices_in_subset,  model, val_dataset, device, loss_fn, rggb=rggb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "641d864b",
   "metadata": {},
   "outputs": [],
   "source": [
    "Train loss: 0.056005 Final image val loss: 0.009073 Time: 45.7s Images: 25\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "256f0de3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import mlflow\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from time import perf_counter\n",
    "\n",
    "\n",
    "def make_conditioning(conditioning, device):\n",
    "    B = conditioning.shape[0]\n",
    "    conditioning_extended = torch.zeros(B, 1).to(device)\n",
    "    conditioning_extended[:, 0] = conditioning[:, 0]\n",
    "    return conditioning_extended\n",
    "\n",
    "\n",
    "def validate_model(\n",
    "    _model, \n",
    "    val_dataset, \n",
    "    _device, \n",
    "    _loss_func, epoch,\n",
    "    iso_values=(100, 400, 1600, 65535), \n",
    "    n_examples=3, \n",
    "    rggb=False,\n",
    "    artifact_dir=\"val_examples\"\n",
    "):\n",
    "    \"\"\"\n",
    "    Run validation over different ISO values and log metrics + sample images to MLflow.\n",
    "\n",
    "    Args:\n",
    "        _model: torch model\n",
    "        val_dataset: dataset with `df.iso` values accessible\n",
    "        _device: torch device\n",
    "        _loss_func: loss function\n",
    "        iso_values: list/tuple of ISO values to evaluate\n",
    "        n_examples: number of images to log per ISO\n",
    "        rggb: whether to use rggb input\n",
    "        artifact_dir: subdirectory to store example images for MLflow\n",
    "    \"\"\"\n",
    "\n",
    "    _model.eval()\n",
    "    os.makedirs(artifact_dir, exist_ok=True)\n",
    "\n",
    "    all_metrics = {}\n",
    "\n",
    "    for iso in iso_values:\n",
    "        # Select subset indices with this ISO\n",
    "        subset_indices = np.array(val_dataset.indices)\n",
    "        mask = val_dataset.dataset.df.iso.values[subset_indices] == iso\n",
    "        matching_indices_in_subset = np.nonzero(mask)[0]\n",
    "\n",
    "        if len(matching_indices_in_subset) == 0:\n",
    "            print(f\"No validation samples for ISO {iso}\")\n",
    "            continue\n",
    "\n",
    "\n",
    "        idxs = matching_indices_in_subset[:n_examples]\n",
    "\n",
    "        total_loss, final_img_loss, duration = visualize(\n",
    "            idxs, _model, val_dataset, _device, _loss_func, rggb=rggb\n",
    "        )\n",
    "\n",
    "\n",
    "        # Save and log a few example denoised images\n",
    "        for i, idx in enumerate(idxs):\n",
    "            row = val_dataset[idx]\n",
    "            noisy = row['noisy'].unsqueeze(0).float().to(_device)\n",
    "            conditioning = make_conditioning(row['conditioning'].float().unsqueeze(0).to(_device), _device)\n",
    "            gt = row['aligned'].unsqueeze(0).float().to(_device)\n",
    "            input = row['rggb' if rggb else 'sparse'].unsqueeze(0).float().to(_device)\n",
    "\n",
    "            with torch.no_grad():\n",
    "                with torch.autocast(device_type=\"mps\", dtype=torch.bfloat16):\n",
    "                    pred = _model(input, conditioning, noisy)\n",
    "\n",
    "            pred_img = apply_gamma_torch(pred[0].cpu().permute(1, 2, 0))\n",
    "            noisy_img = apply_gamma_torch(noisy[0].cpu().permute(1, 2, 0))\n",
    "            gt_img = apply_gamma_torch(gt[0].cpu().permute(1, 2, 0))\n",
    "\n",
    "            fig, axs = plt.subplots(1, 3, figsize=(15, 5))\n",
    "            axs[0].imshow(noisy_img)\n",
    "            axs[0].set_title(f\"Noisy (ISO {iso})\")\n",
    "            axs[1].imshow(pred_img)\n",
    "            axs[1].set_title(\"Denoised\")\n",
    "            axs[2].imshow(gt_img)\n",
    "            axs[2].set_title(\"Ground Truth\")\n",
    "            for ax in axs: ax.axis('off')\n",
    "\n",
    "            img_path = os.path.join(artifact_dir, f\"iso{iso}_example{i}.png\")\n",
    "            plt.savefig(img_path, bbox_inches=\"tight\")\n",
    "            plt.close(fig)\n",
    "\n",
    "            mlflow.log_artifact(img_path, artifact_path=f\"{artifact_dir}/ISO_{iso}\")\n",
    "\n",
    "        # Log metrics per ISO\n",
    "        mlflow.log_metrics({\n",
    "            f\"val_loss_ISO_{iso}\": total_loss,\n",
    "            f\"final_img_loss_ISO_{iso}\": final_img_loss,\n",
    "            f\"val_duration_ISO_{iso}\": duration\n",
    "        }, step=epoch)\n",
    "\n",
    "    # Also log a summary table\n",
    "    print(\"\\nValidation Summary:\")\n",
    "    for iso, metrics in all_metrics.items():\n",
    "        print(f\"ISO {iso}: val_loss={metrics['val_loss']:.6f}, \"\n",
    "              f\"final_img_loss={metrics['final_image_loss']:.6f}, \"\n",
    "              f\"time={metrics['duration']:.1f}s\")\n",
    "\n",
    "    return all_metrics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15ff7052",
   "metadata": {},
   "outputs": [],
   "source": [
    "with mlflow.start_run(run_name=run_config['run_name']) as run:\n",
    "    metrics = validate_model(model, val_dataset, device, loss_fn, rggb=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db26581d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class SwiGLU(nn.Module):\n",
    "\n",
    "    def __init__(self, input_dim, hidden_dim, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.w1 = nn.Conv2d(input_dim, hidden_dim, 1, 1, 0, 1)\n",
    "        self.w2 = nn.Conv2d(input_dim, hidden_dim, 1, 1, 0, 1)\n",
    "        self.w3 = nn.Conv2d(hidden_dim, input_dim, 1, 1, 0, 1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        gate = F.silu(self.w1(x)) \n",
    "        value = self.w2(x)\n",
    "        x = gate * value \n",
    "        \n",
    "        x = self.w3(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc387d19",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConditionedChannelAttention(nn.Module):\n",
    "    def __init__(self, dims, cat_dims):\n",
    "        super().__init__()\n",
    "        in_dim = dims + cat_dims\n",
    "        self.mlp = nn.Sequential(nn.Linear(in_dim, dims))\n",
    "        self.pool = nn.AdaptiveAvgPool2d(1)\n",
    "\n",
    "    def forward(self, x, conditioning):\n",
    "        pool = self.pool(x)\n",
    "        conditioning = conditioning.unsqueeze(-1).unsqueeze(-1)\n",
    "        cat_channels = torch.cat([pool, conditioning], dim=1)\n",
    "        cat_channels = cat_channels.permute(0, 2, 3, 1)\n",
    "        ca = self.mlp(cat_channels).permute(0, 3, 1, 2)\n",
    "\n",
    "        return ca"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86fdd311",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "class SwiGLU(nn.Module):\n",
    "\n",
    "    def __init__(self, input_dim, hidden_dim, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.w1 = nn.Conv2d(input_dim, hidden_dim, 1, 1, 0, 1)\n",
    "        self.w2 = nn.Conv2d(input_dim, hidden_dim, 1, 1, 0, 1)\n",
    "        self.w3 = nn.Conv2d(hidden_dim, input_dim, 1, 1, 0, 1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        gate = F.silu(self.w1(x)) \n",
    "        value = self.w2(x)\n",
    "        x = gate * value \n",
    "        \n",
    "        x = self.w3(x)\n",
    "        return x\n",
    "    \n",
    "class AttnBlock(nn.Module):\n",
    "    def __init__(self, c, FFN_Expand=2, drop_out_rate=0.0, cond_chans=0):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.dw = nn.Conv2d(\n",
    "            in_channels=c,\n",
    "            out_channels=c,\n",
    "            kernel_size=3,\n",
    "            padding=1,\n",
    "            stride=1,\n",
    "            groups=c,\n",
    "            bias=True,\n",
    "        )\n",
    "\n",
    "        self.sca = ConditionedChannelAttention(c, cond_chans)\n",
    "\n",
    "        self.norm = nn.GroupNorm(1, c)\n",
    "        \n",
    "        self.swiglu = SwiGLU(c, int(c *  FFN_Expand))\n",
    "        self.alpha = nn.Parameter(torch.zeros(1, c, 1, 1))\n",
    "        self.beta = nn.Parameter(torch.zeros(1, c, 1, 1))\n",
    "\n",
    "\n",
    "    def forward(self, input):\n",
    "        inp = input[0]\n",
    "        cond = input[1]\n",
    "\n",
    "        x = self.dw(inp)\n",
    "        x = self.sca(x, cond) * x\n",
    "        y = self.norm(inp + self.alpha * x )\n",
    "\n",
    "\n",
    "        x = self.swiglu(y)\n",
    "        x = y + self.beta * x\n",
    "        return (x, cond)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "990a5993",
   "metadata": {},
   "outputs": [],
   "source": [
    "block = AttnBlock(64, 2, cond_chans=1)\n",
    "block((torch.rand(1, 64, 32, 32), torch.rand(1, 1)))[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b460abf",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NAFBlock0(nn.Module):\n",
    "    def __init__(self, c, DW_Expand=2, FFN_Expand=2, drop_out_rate=0.0, cond_chans=0):\n",
    "        super().__init__()\n",
    "        dw_channel = c * DW_Expand\n",
    "        self.conv1 = nn.Conv2d(\n",
    "            in_channels=c,\n",
    "            out_channels=dw_channel,\n",
    "            kernel_size=1,\n",
    "            padding=0,\n",
    "            stride=1,\n",
    "            groups=1,\n",
    "            bias=True,\n",
    "        )\n",
    "        self.conv2 = nn.Conv2d(\n",
    "            in_channels=dw_channel,\n",
    "            out_channels=dw_channel,\n",
    "            kernel_size=3,\n",
    "            padding=1,\n",
    "            stride=1,\n",
    "            groups=dw_channel,\n",
    "            bias=True,\n",
    "        )\n",
    "        self.conv3 = nn.Conv2d(\n",
    "            in_channels=dw_channel // 2,\n",
    "            out_channels=c,\n",
    "            kernel_size=1,\n",
    "            padding=0,\n",
    "            stride=1,\n",
    "            groups=1,\n",
    "            bias=True,\n",
    "        )\n",
    "\n",
    "        # Simplified Channel Attention\n",
    "        self.sca = ConditionedChannelAttention(dw_channel // 2, cond_chans)\n",
    "\n",
    "        # SimpleGate\n",
    "        self.sg = SimpleGate()\n",
    "\n",
    "        ffn_channel = FFN_Expand * c\n",
    "        self.conv4 = nn.Conv2d(\n",
    "            in_channels=c,\n",
    "            out_channels=ffn_channel,\n",
    "            kernel_size=1,\n",
    "            padding=0,\n",
    "            stride=1,\n",
    "            groups=1,\n",
    "            bias=True,\n",
    "        )\n",
    "        self.conv5 = nn.Conv2d(\n",
    "            in_channels=ffn_channel // 2,\n",
    "            out_channels=c,\n",
    "            kernel_size=1,\n",
    "            padding=0,\n",
    "            stride=1,\n",
    "            groups=1,\n",
    "            bias=True,\n",
    "        )\n",
    "\n",
    "        # self.grn = GRN(ffn_channel // 2)\n",
    "\n",
    "        self.norm1 = LayerNorm2d(c)\n",
    "        self.norm2 = LayerNorm2d(c)\n",
    "\n",
    "        self.dropout1 = (\n",
    "            nn.Dropout(drop_out_rate) if drop_out_rate > 0.0 else nn.Identity()\n",
    "        )\n",
    "        self.dropout2 = (\n",
    "            nn.Dropout(drop_out_rate) if drop_out_rate > 0.0 else nn.Identity()\n",
    "        )\n",
    "\n",
    "        self.beta = nn.Parameter(torch.zeros((1, c, 1, 1)), requires_grad=True)\n",
    "        self.gamma = nn.Parameter(torch.zeros((1, c, 1, 1)), requires_grad=True)\n",
    "\n",
    "    def forward(self, input):\n",
    "        inp = input[0]\n",
    "        cond = input[1]\n",
    "\n",
    "        x = inp\n",
    "\n",
    "        x = self.norm1(x)\n",
    "        x = self.conv1(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.sg(x)\n",
    "        x = x * self.sca(x, cond)\n",
    "        x = self.conv3(x)\n",
    "\n",
    "        x = self.dropout1(x)\n",
    "\n",
    "        y = inp + x * self.beta\n",
    "\n",
    "        # Channel Mixing\n",
    "        x = self.conv4(self.norm2(y))\n",
    "        x = self.sg(x)\n",
    "        # x = self.grn(x)\n",
    "        x = self.conv5(x)\n",
    "\n",
    "        x = self.dropout2(x)\n",
    "\n",
    "        return (y + x * self.gamma, cond)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "OnSight",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
