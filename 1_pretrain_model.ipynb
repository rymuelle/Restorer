{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6351e77",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2043dc8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.training.SmallRawDataset import SmallRawDataset\n",
    "# from src.Restorer.Cond_NAF import make_model, make_full_model, make_full_model_RGGB\n",
    "\n",
    "from src.training.ShadowAwareLoss import ShadowAwareLoss\n",
    "from src.training.VGGFeatureExtractor import VGGFeatureExtractor\n",
    "from src.training.sparse_loop import train_one_epoch, visualize\n",
    "from src.training.rggb_loop import train_one_epoch_rggb, visualize\n",
    "\n",
    "\n",
    "from src.training.utils import apply_gamma_torch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba20b866",
   "metadata": {},
   "outputs": [],
   "source": [
    "device= 'mps'\n",
    "\n",
    "batch_size = 2\n",
    "lr = 1e-4 * batch_size / 4\n",
    "# lr = 1e-3 * batch_size / 32\n",
    "clipping =  1e-2\n",
    "\n",
    "num_epochs = 75\n",
    "val_split = 0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15f16fa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = SmallRawDataset('/Volumes/EasyStore/RAWNIND/JPEGs/Cropped_JPEG_high_quality/', 'align.csv', crop_size=256)\n",
    "\n",
    "\n",
    "# Split dataset into train and val\n",
    "val_size = int(len(dataset) * val_split)\n",
    "train_size = len(dataset) - val_size\n",
    "torch.manual_seed(42)  # For reproducibility\n",
    "train_dataset, val_dataset = random_split(dataset, [train_size, val_size])\n",
    "# Set the validation dataset to use the same crops\n",
    "val_dataset = copy.deepcopy(val_dataset)\n",
    "val_dataset.dataset.validation = True\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=False, num_workers=0)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a219ab5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_name = '/Volumes/EasyStore/models/Cond_NAF_original_null.pt'\n",
    "# model = make_full_model_RGGB(model_name=None)\n",
    "# model = model.to(device)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7669fd6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # model_name = '/Volumes/EasyStore/models/Restorer_train_vgg_relu_300_full.pt'\n",
    "# # model = make_full_model(model_name=model_name)\n",
    "# # model = model.to(device)\n",
    "\n",
    "# model_name = '/Volumes/EasyStore/models/Restorer_train_vgg_relu_300_full_RGGB.pt'\n",
    "# model_name = '/Volumes/EasyStore/models/Restorer_train_vgg_relu_0_full_RGGB.pt'\n",
    "\n",
    "# model = make_full_model_RGGB(model_name=model_name)\n",
    "# model = model.to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "086fbb3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.Restorer.Cond_NAF import make_model, make_full_model, make_full_model_RGGB\n",
    "\n",
    "model_name = '/Volumes/EasyStore/models/Restorer_train_vgg_relu_300_full_RGGB.pt'\n",
    "model_name = '/Volumes/EasyStore/models/Restorer_train_vgg_relu_0_full_RGGB.pt'\n",
    "\n",
    "model = make_full_model_RGGB(model_name=model_name)\n",
    "model = model.to(device)\n",
    "\n",
    "\n",
    "from src.Restorer.Cond_CHASPA import  make_full_model_RGGB\n",
    "\n",
    "\n",
    "model_name = '/Volumes/EasyStore/models/Restorer_train_vgg_relu_0_full_RGGB_CHASPA.pt'\n",
    "\n",
    "model = make_full_model_RGGB(model_name=model_name)\n",
    "model = model.to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8012a124",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_name = '/Volumes/EasyStore/models/Restorer_train_vgg_relu_300_full.pt'\n",
    "# model = make_full_model(model_name=model_name)\n",
    "# # model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a666123",
   "metadata": {},
   "outputs": [],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6af0f3a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "vfe = VGGFeatureExtractor(config=((1, 64), (1, 128), (1, 256), (1, 512), (1, 512),), \n",
    "                          feature_layers=[14], \n",
    "                          activation=nn.ReLU\n",
    "                          )\n",
    "vfe = vfe.to(device)\n",
    "\n",
    "loss_fn = ShadowAwareLoss(\n",
    "    alpha=0.2,\n",
    "    beta=5.0,\n",
    "    l1_weight=0.16,\n",
    "    ssim_weight=0.84,\n",
    "    tv_weight=0.0,\n",
    "    vgg_loss_weight=0,\n",
    "    apply_gamma_fn=apply_gamma_torch,\n",
    "    vgg_feature_extractor=vfe,\n",
    "    device=device,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bc86a06",
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(num_epochs):\n",
    "    train_one_epoch(epoch, model, optimizer, model_name, train_loader, device, loss_fn, clipping, log_interval = 10, sleep=0.0, rggb=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "OnSight",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
