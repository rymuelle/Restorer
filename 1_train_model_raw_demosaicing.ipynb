{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6351e77",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "import copy\n",
    "import mlflow\n",
    "import mlflow.pytorch\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2043dc8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.training.DemosaicingDataset import DemosaicingDataset\n",
    "from src.training.losses.ShadowAwareLoss import ShadowAwareLoss\n",
    "from src.training.VGGFeatureExtractor import VGGFeatureExtractor\n",
    "from src.training.train_loop import train_one_epoch, visualize\n",
    "from src.training.utils import apply_gamma_torch\n",
    "from src.training.load_config import load_config\n",
    "from src.Restorer.Cond_NAF import  make_full_model_RGGB_Demosaicing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0464c98",
   "metadata": {},
   "outputs": [],
   "source": [
    "run_config = load_config('config_demosaicing.yaml')\n",
    "dataset_path = Path(run_config['cropped_raw_subdir'])\n",
    "align_csv = dataset_path / run_config['secondary_align_csv']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba20b866",
   "metadata": {},
   "outputs": [],
   "source": [
    "device=run_config['device']\n",
    "\n",
    "batch_size = run_config['batch_size']\n",
    "lr = run_config['lr_base'] * batch_size\n",
    "clipping =  run_config['clipping']\n",
    "\n",
    "num_epochs = run_config['num_epochs_pretraining']\n",
    "cosine_annealing = run_config['cosine_annealing']\n",
    "\n",
    "val_split = run_config['val_split']\n",
    "crop_size = run_config['crop_size']\n",
    "experiment = run_config['mlflow_experiment']\n",
    "mlflow_path = run_config['mlflow_path']\n",
    "colorspace = run_config['colorspace']\n",
    "iso_range = run_config['iso_range']\n",
    "\n",
    "rggb = True\n",
    "mlflow.set_tracking_uri(f\"file://{mlflow_path}\")\n",
    "mlflow.set_experiment(experiment)\n",
    "\n",
    "params = {**run_config}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9a26124",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_params = run_config['model_params']\n",
    "rggb = model_params['rggb']\n",
    "\n",
    "model =  make_full_model_RGGB_Demosaicing(model_params, model_name=None)\n",
    "model = model.to(device)\n",
    "\n",
    "params = {**run_config, **model_params}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15f16fa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = DemosaicingDataset(dataset_path, align_csv, colorspace, output_crop_size=crop_size, downsample_factor=4)\n",
    "dataset.df = dataset.df[~dataset.df.bayer_path.str.contains('crw')]\n",
    "dataset.df = dataset.df[~dataset.df.bayer_path.str.contains('dng_bayer')]\n",
    "dataset.df = dataset.df[(dataset.df.iso >= iso_range[0]) & (dataset.df.iso <= iso_range[1])]\n",
    "print(len(dataset.df ))\n",
    "# Split dataset into train and val\n",
    "val_size = int(len(dataset) * val_split)\n",
    "train_size = len(dataset) - val_size\n",
    "torch.manual_seed(42)  # For reproducibility\n",
    "train_dataset, val_dataset = random_split(dataset, [train_size, val_size])\n",
    "# Set the validation dataset to use the same crops\n",
    "val_dataset = copy.deepcopy(val_dataset)\n",
    "val_dataset.dataset.validation = True\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=0)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=True, num_workers=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6af0f3a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "if cosine_annealing:\n",
    "    sched = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, num_epochs,eta_min=lr*1e-6)\n",
    "else:\n",
    "    sched = None\n",
    "    \n",
    "vfe = VGGFeatureExtractor(config=((1, 64), (1, 128), (1, 256), (1, 512), (1, 512),), \n",
    "                          feature_layers=[14], \n",
    "                          activation=nn.ReLU\n",
    "                          )\n",
    "vfe = vfe.to(device)\n",
    "\n",
    "loss_fn = ShadowAwareLoss(\n",
    "    alpha=run_config['alpha'],\n",
    "    beta=run_config['beta'],\n",
    "    l1_weight=run_config['l1_weight'],\n",
    "    ssim_weight=run_config['ssim_weight'],\n",
    "    tv_weight=run_config['tv_weight'],\n",
    "    vgg_loss_weight=run_config['vgg_loss_weight'],\n",
    "    percept_loss_weight=run_config['percept_loss_weight'],\n",
    "    apply_gamma_fn=apply_gamma_torch,\n",
    "    vgg_feature_extractor=vfe,\n",
    "    device=device,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4616a32",
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import perf_counter\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from src.training.utils import apply_gamma_torch\n",
    "import mlflow\n",
    "\n",
    "def make_conditioning(conditioning, device):\n",
    "    B = conditioning.shape[0]\n",
    "    conditioning_extended = torch.zeros(B, 1).to(device)\n",
    "    conditioning_extended[:, 0] = conditioning[:, 0]\n",
    "    return conditioning_extended\n",
    "\n",
    "\n",
    "def train_one_epoch(epoch, _model, _optimizer, _loader, _device, _loss_func, _clipping, \n",
    "                    log_interval = 10, sleep=0.0, rggb=False,\n",
    "                    max_batches=0):\n",
    "    _model.train()\n",
    "    total_loss, n_images, total_l1_loss = 0.0, 0, 0.0\n",
    "    start = perf_counter()\n",
    "    pbar = tqdm(enumerate(_loader), total=len(_loader), desc=f\"Train Epoch {epoch}\")\n",
    "\n",
    "    for batch_idx, (output) in pbar:\n",
    "        conditioning = output['conditioning'].float().to(_device)\n",
    "        gt = output['ground_truth'].float().to(_device)\n",
    "        input = output['cfa_sparse'].float().to(_device)\n",
    "        if rggb:\n",
    "            input = output['cfa_rggb'].float().to(_device)\n",
    "        conditioning = make_conditioning(conditioning, _device)\n",
    "\n",
    "        _optimizer.zero_grad(set_to_none=True)\n",
    "        pred = _model(input, conditioning) \n",
    "\n",
    "        loss = _loss_func(pred, gt)\n",
    "        _optimizer.zero_grad(set_to_none=True)\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(_model.parameters(), _clipping)\n",
    "        _optimizer.step()\n",
    "\n",
    "        total_loss +=  float(loss.detach().cpu())\n",
    "        n_images += gt.shape[0]\n",
    "\n",
    "        # Testing final image quality\n",
    "        final_image_loss = float(nn.functional.l1_loss(pred, gt).detach().cpu())\n",
    "        total_l1_loss += final_image_loss\n",
    "        del loss, pred, final_image_loss\n",
    "        torch.mps.empty_cache() \n",
    "\n",
    "        if (batch_idx + 1) % log_interval == 0:\n",
    "                pbar.set_postfix({\"loss\": f\"{total_loss/n_images:.4f}\"})\n",
    "\n",
    "        if (max_batches > 0) and (batch_idx+1 > max_batches): break\n",
    "        time.sleep(sleep)\n",
    "\n",
    "    train_time = perf_counter()-start\n",
    "    print(f\"[Epoch {epoch}] \"\n",
    "                f\"Train loss: {total_loss/n_images:.6f} \"\n",
    "                f\"L1 loss: {total_l1_loss/n_images:.6f} \"\n",
    "                f\"Time: {train_time:.1f}s \"\n",
    "                f\"Images: {n_images}\")\n",
    "    mlflow.log_metric(\"train_loss\", total_loss/n_images, step=epoch)\n",
    "    mlflow.log_metric(\"l1_loss\", total_l1_loss/n_images, step=epoch)\n",
    "    mlflow.log_metric(\"epoch_duration_s\", train_time, step=epoch)\n",
    "    mlflow.log_metric(\"learning_rate\", _optimizer.param_groups[0]['lr'], step=epoch)\n",
    "\n",
    "    return total_loss / max(1, n_images), perf_counter()-start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bc86a06",
   "metadata": {},
   "outputs": [],
   "source": [
    "with mlflow.start_run(run_name=run_config['run_name']) as run:\n",
    "    mlflow.log_params(params)\n",
    "    for epoch in range(num_epochs):\n",
    "        train_one_epoch(epoch, model, optimizer, train_loader, device, loss_fn, clipping, \n",
    "                        log_interval = 10, sleep=0.0, rggb=rggb, max_batches=0)\n",
    "        if cosine_annealing:\n",
    "            sched.step()\n",
    "        \n",
    "    mlflow.pytorch.log_model(\n",
    "        pytorch_model=model,\n",
    "        name=run_config['run_path'],\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a775d31",
   "metadata": {},
   "outputs": [],
   "source": [
    "run.info.run_id"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "OnSight",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
